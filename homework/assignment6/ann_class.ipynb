{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d8313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90f8f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Module(ABC):\n",
    "    \"\"\"All the methods you can do on a layer\"\"\"\n",
    "    # @classmethod\n",
    "    @abstractmethod  \n",
    "    def forward(self, *args, **kwargs):\n",
    "        ...\n",
    "    def __call__(self, inputs): ## instead of inputs I could have *args, **kwargs\n",
    "        return self.forward(inputs)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b4476799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    import numpy as np\n",
    "    \"\"\"Create layers\"\"\"\n",
    "    def __init__(self, num_nodes_intput_layer, num_nodes_output_layer):\n",
    "        \"\"\"Dimensions: output is rows, input is columns.\"\"\"    \n",
    "        super().__setattr__(\"layers\", {}) ## call the original __setattr__\n",
    "        \n",
    "        self.weights = np.random.rand(num_nodes_output_layer, num_nodes_intput_layer) * 0.01\n",
    "        self.bias = np.random.rand(num_nodes_output_layer,1) * 0.01 \n",
    "        self.logits = np.empty(0)\n",
    "        self.sigmoids = np.empty(0)\n",
    "        super().__init__()    \n",
    "        \n",
    "    def __setattr__(self, name: str, value: Any) -> None:\n",
    "        # print(f\"name {name}, value {value}\")\n",
    "        super().__setattr__(name, value) ## do normal __setattr__ assignment for the layers\n",
    "\n",
    "        if name != \"layers\": ## for the weights and biases add it to the layer dict\n",
    "            self.layers[name] = value\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Dimensions: output is rows, input is columns.\\nweights: {self.weights.shape}, bias: {self.bias.shape}\"\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        layer = np.concatenate([self.bias, self.weights], axis = 1)\n",
    "        # print(layer)\n",
    "        ## linear combination w0x0 + w1x1 + w2x2\n",
    "        linear_combinations = []\n",
    "\n",
    "        inputs = np.c_[ np.ones(inputs.shape[0]), inputs  ]    ## add bias to the inputs\n",
    "\n",
    "        linear_combinations = np.matmul(inputs, layer.T) ## logits or z\n",
    "        \n",
    "        sigmoids = 1/(  1 + np.exp( -linear_combinations ) ) ## compute a\n",
    "\n",
    "        self.logits = np.append(self.logits, linear_combinations)\n",
    "        self.sigmoids = np.append(self.sigmoids, sigmoids)\n",
    "        return sigmoids\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5b93b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52221323, 0.51199034],\n",
       "       [0.52059328, 0.51084673],\n",
       "       [0.52061008, 0.51104628],\n",
       "       [0.52020993, 0.51101378],\n",
       "       [0.52223498, 0.51214689],\n",
       "       [0.52432812, 0.51375844],\n",
       "       [0.52111486, 0.5117448 ],\n",
       "       [0.52178647, 0.51184427],\n",
       "       [0.51927592, 0.51038087],\n",
       "       [0.52063901, 0.51094813],\n",
       "       [0.52336258, 0.5126747 ],\n",
       "       [0.52138144, 0.51185476],\n",
       "       [0.5201587 , 0.51057488],\n",
       "       [0.51899879, 0.50997637],\n",
       "       [0.52485812, 0.51316439],\n",
       "       [0.52610924, 0.51472616],\n",
       "       [0.5242213 , 0.51330422],\n",
       "       [0.52243182, 0.51221061],\n",
       "       [0.52451939, 0.51348474],\n",
       "       [0.52317126, 0.51294842],\n",
       "       [0.5227033 , 0.51227758],\n",
       "       [0.52315226, 0.51296059],\n",
       "       [0.52126464, 0.51148643],\n",
       "       [0.52247395, 0.51257566],\n",
       "       [0.52146159, 0.51219547],\n",
       "       [0.52086263, 0.51112544],\n",
       "       [0.5222504 , 0.51239838],\n",
       "       [0.52245579, 0.51215545],\n",
       "       [0.52219147, 0.51183379],\n",
       "       [0.52069024, 0.51138702],\n",
       "       [0.52066848, 0.51123045],\n",
       "       [0.52308702, 0.51249097],\n",
       "       [0.52366262, 0.51318367],\n",
       "       [0.52473926, 0.51365302],\n",
       "       [0.52085767, 0.51116842],\n",
       "       [0.52123105, 0.51108734],\n",
       "       [0.52304985, 0.51208294],\n",
       "       [0.52180051, 0.51187508],\n",
       "       [0.51948691, 0.5104754 ],\n",
       "       [0.52200233, 0.51189582],\n",
       "       [0.52218926, 0.5120455 ],\n",
       "       [0.51825746, 0.50929034],\n",
       "       [0.51996231, 0.51089164],\n",
       "       [0.52292515, 0.51304697],\n",
       "       [0.52349666, 0.51362288],\n",
       "       [0.52059603, 0.51101547],\n",
       "       [0.52297941, 0.51284173],\n",
       "       [0.52042089, 0.51110831],\n",
       "       [0.52314677, 0.51262316],\n",
       "       [0.52152213, 0.5115226 ],\n",
       "       [0.52910168, 0.51873386],\n",
       "       [0.5279732 , 0.51841799],\n",
       "       [0.5289205 , 0.51892148],\n",
       "       [0.52332444, 0.51507471],\n",
       "       [0.52726611, 0.51775122],\n",
       "       [0.52507708, 0.51678536],\n",
       "       [0.52826663, 0.51902145],\n",
       "       [0.5214242 , 0.51351812],\n",
       "       [0.52728235, 0.51757043],\n",
       "       [0.52381906, 0.51585886],\n",
       "       [0.52074294, 0.51296451],\n",
       "       [0.52634019, 0.51740409],\n",
       "       [0.52351026, 0.5144637 ],\n",
       "       [0.52644918, 0.51764651],\n",
       "       [0.52485852, 0.5159202 ],\n",
       "       [0.52813764, 0.51803097],\n",
       "       [0.52577321, 0.51759006],\n",
       "       [0.52429304, 0.51551431],\n",
       "       [0.52516786, 0.51623534],\n",
       "       [0.52355154, 0.51498836],\n",
       "       [0.52763027, 0.51916118],\n",
       "       [0.52580646, 0.51642387],\n",
       "       [0.52620269, 0.51736482],\n",
       "       [0.52577489, 0.51699828],\n",
       "       [0.526771  , 0.51712691],\n",
       "       [0.5276847 , 0.51777152],\n",
       "       [0.52774799, 0.5179126 ],\n",
       "       [0.52871539, 0.5191642 ],\n",
       "       [0.52639854, 0.51758814],\n",
       "       [0.5236795 , 0.51457356],\n",
       "       [0.52307147, 0.51461526],\n",
       "       [0.52282619, 0.51428151],\n",
       "       [0.52467665, 0.51572763],\n",
       "       [0.5263023 , 0.51807331],\n",
       "       [0.52534178, 0.51748704],\n",
       "       [0.52780379, 0.51884792],\n",
       "       [0.52843601, 0.51859152],\n",
       "       [0.52515741, 0.515941  ],\n",
       "       [0.52522949, 0.51669577],\n",
       "       [0.52379953, 0.51549075],\n",
       "       [0.52392536, 0.51593267],\n",
       "       [0.52665989, 0.51774097],\n",
       "       [0.52446585, 0.51563314],\n",
       "       [0.52140245, 0.5133616 ],\n",
       "       [0.52454375, 0.51618532],\n",
       "       [0.52525345, 0.51664065],\n",
       "       [0.52523445, 0.51665282],\n",
       "       [0.52633966, 0.51702389],\n",
       "       [0.52223202, 0.51370883],\n",
       "       [0.52497028, 0.51633132]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "target = data.target[(data.target == 0) | (data.target == 1)]\n",
    "data = data.data[(data.target == 0) | (data.target == 1)]\n",
    "\n",
    "layer1 = Linear(4,2)\n",
    "\n",
    "layer1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "    super().__setattr__(\"layers\", {})\n",
    "\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Layer):\n",
    "            self.layers[name] = value\n",
    "        super().__setattr__(name, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c341bf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yippe'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dog:\n",
    "    def __init__(self, bark):\n",
    "        self.bark = bark\n",
    "\n",
    "d = Dog(\"yippe\")\n",
    "d.bark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c2b323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        return x * 2  # example method\n",
    "\n",
    "class Linear(Layer):\n",
    "    import numpy as np\n",
    "    \"\"\"Create layers\"\"\"\n",
    "    def __init__(self, num_nodes_intput_layer, num_nodes_output_layer):\n",
    "        \"\"\"Dimensions: output is rows, input is columns.\"\"\"\n",
    "        super().__init__()        \n",
    "        super().__setattr__(\"layers\", {}) ## call the original __setattr__\n",
    "        \n",
    "        self.weights = np.random.rand(num_nodes_output_layer, num_nodes_intput_layer) * 0.01\n",
    "        self.bias = np.random.rand(num_nodes_output_layer,1) * 0.01 \n",
    "\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\n",
    "        # print(f\"name {name}, value {value}\")\n",
    "        super().__setattr__(name, value) ## do normal __setattr__ assignment for the layers\n",
    "\n",
    "        if name != \"layers\": ## for the weights and biases add it to the layer dict\n",
    "            self.layers[name] = value\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Dimensions: output is rows, input is columns.\\nweights: {self.weights.shape}, bias: {self.bias.shape}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ed24c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Linear(2,3)\n",
    "a.forward(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machinelearning)",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
