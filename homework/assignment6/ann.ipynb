{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef77b431",
   "metadata": {},
   "source": [
    "![\"question\"](question.jpg)\n",
    "\n",
    "\n",
    "\n",
    "between layer 1 and 2:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  b_{10} & w_{11} & w_{21} \\\\\n",
    "  b_{20} & w_{12} & w_{22} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where b = bias and w = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb473e93",
   "metadata": {},
   "source": [
    "$x = [0.1,0.2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18bd8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e67c6",
   "metadata": {},
   "source": [
    "input is columns, output is rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4efc0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(num_nodes_input_layer, num_nodes_output_layer):\n",
    "    \"\"\"Dimensions: output is rows, input is columns.\"\"\"\n",
    "    seed(1)\n",
    "    weights = np.random.rand(num_nodes_output_layer, num_nodes_input_layer + 1)\n",
    "    # biases = np.ones((weights.shape[0], 1))\n",
    "\n",
    "    # Stack the original array and the column of ones\n",
    "    # return np.column_stack((biases, weights ))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ae8ad",
   "metadata": {},
   "source": [
    "# Forward propogation to make predictions\n",
    "\n",
    "![\"how to apply the weights\"](apply_weights.png)\n",
    "\n",
    "$$ output = \n",
    "\\begin{bmatrix}\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:1 & Neuron\\:2\\:applied\\:to\\:sample\\:1 \\\\\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:2 & Neuron\\:2\\:applied\\:to\\:sample\\:2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5fc97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward(inputs, layer):\n",
    "    ## linear combination w0x0 + w1x1 + w2x2\n",
    "    linear_combinations = []\n",
    "\n",
    "    inputs = np.c_[ np.ones(x.shape[1]), x  ]    ## add bias to the inputs\n",
    "    # print(inputs)\n",
    "    # print(layer)\n",
    "\n",
    "    linear_combinations = np.matmul(inputs, layer.T) ## logits\n",
    "    \n",
    "    # print(linear_combinations)\n",
    "\n",
    "    sigmoids = 1/(  1 + np.exp( -linear_combinations ) ) ## compute a\n",
    "    # print(sigmoids)\n",
    "\n",
    "    return sigmoids, linear_combinations\n",
    "\n",
    "\n",
    "# def sigmoid(inputs, layer):\n",
    "\n",
    "#     linear_combinations = np.matmul(inputs, layer.T) ## logits\n",
    "#     return 1/(  1 + np.exp( -linear_combinations ) ) ## compute a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64766b",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "*m is number of samples*\n",
    "\n",
    "*k is total number of output units*\n",
    "$$\n",
    " J(θ)= - \\frac{1}{m} {\\sum^m_{i=1}\\sum^k_{k=1} [y^{(i)}_kln(\\frac{1}{1+e^{xθ^T}})_k + (1-y^{(i)}_k)ln(1-(\\frac{1}{1+e^{xθ^T}})_k)]} \n",
    "$$\n",
    "$$\n",
    " = \\frac{1}{m} {\\sum^m_{i=1}\\sum^k_{k=1} [y^{(i)}_kln(a^{(L)}_k + (1-y^{(i)}_k)ln(1-a^{(L)}_k)]}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "$$  \n",
    "output\\_sigmoids = \n",
    "\\begin{bmatrix}\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:1 & Neuron\\:2\\:applied\\:to\\:sample\\:1 \\\\\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:2 & Neuron\\:2\\:applied\\:to\\:sample\\:2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(output_sigmoids, target):\n",
    "    \n",
    "    ## m is number of samples\n",
    "    ## k is total number of output units\n",
    "\n",
    "    log_sigmoids = np.log(output_sigmoids)\n",
    "    y_times_log_sigmoids = np.matmul(log_sigmoids, target.T)\n",
    "    \n",
    "    one_minus_log_sigmoids = 1 - np.log(output_sigmoids)\n",
    "    one_minus_y_times_log_sigmoids = np.matmul(one_minus_log_sigmoids, (1 - target).T)\n",
    "    return sum(sum((y_times_log_sigmoids + one_minus_y_times_log_sigmoids)))/len(target) ## sum across the output units and sum across the samples\n",
    "    # for sample in target:\n",
    "    #     for row in output_sigmoids:\n",
    "            \n",
    "    #         print(row, sample)\n",
    "    #     print(\"_____________\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64301d0d",
   "metadata": {},
   "source": [
    "# Backpropogation\n",
    "\n",
    "want: $\\frac{∂J(θ)}{∂θ^{(l)}_{ij}}$ for $l=1,2$\n",
    "\n",
    "for each\n",
    "\n",
    "$$δ^{(L)} = \\begin{bmatrix}\n",
    "  a^{(L)}_{1} - y_{11} \\\\\n",
    "  a^{(L)}_{2} - y_{12} \\\\\n",
    "  a^{(L)}_1 - y_{21} \\\\ \n",
    "  a^{(L)}_1 - y_{22} \\\\ \n",
    "\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$y_{11}$ is sample $1$, for target $1 $\n",
    "\n",
    "$$δ^{(L)} * \\begin{bmatrix} a^{(L-1)}_{0}  a^{(L-1)}_{1} a^{(L-1)}_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "  \\frac{∂J(θ)}{∂θ^{(L-1)}_{10}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{11}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{12}} \\\\\n",
    "  \\\\\n",
    "  \\frac{∂J(θ)}{∂θ^{(L-1)}_{20}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{21}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{22}} \\\\\n",
    "\n",
    "\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "slides 37, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7755d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_backpropogation(output_sigmoids, y, previous_sigmoids):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    delta = (output_sigmoids - y)#.reshape(-1,1)\n",
    "\n",
    "    # print(delta)\n",
    "    # print(\"------------\")\n",
    "\n",
    "    gradients = np.matmul(delta, previous_sigmoids.T)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def step(weights, gradients, lr = 0.001):\n",
    "    return weights - (gradients * lr)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b39a4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.1,0.2],\n",
    "      [0.3,0.8]]) # x1, x2\n",
    "\n",
    "y = np.array([[1,0],[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ecfee82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the layers\n",
    "# seed(1)\n",
    "# input = Linear(2,2)\n",
    "seed(1)\n",
    "hidden_layer1 = Linear(2,2)\n",
    "seed(1)\n",
    "output_layer = Linear(2,2)\n",
    "\n",
    "# hidden_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoids2, a2 = compute_forward(x, input)\n",
    "\n",
    "# sigmoids3, a3 = compute_forward(a2, hidden_layer1)\n",
    "\n",
    "sigmoids2, a2 = compute_forward(x, hidden_layer1)\n",
    "\n",
    "output_sigmoids, a_output = compute_forward(a2, output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679f711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4200387428707105"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binary_cross_entropy_loss(output_sigmoids, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7c6490bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.27959399  0.56641099]\n",
      " [-0.22277153 -0.32431642]]\n",
      "------------\n",
      "[[ 0.1810478   0.19610792]\n",
      " [-0.36623869 -0.39345878]]\n"
     ]
    }
   ],
   "source": [
    "compute_backpropogation(output_sigmoids, y, sigmoids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ab6b669f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54628512, 0.29569979, 0.08579499],\n",
       "       [0.81239163, 0.15836378, 0.81902348]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457fdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13692008, 0.4647963 ],\n",
       "       [0.65395656, 0.00602726]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(inputs):\n",
    "    inputs = in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c12b3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4310957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # self.input = nn.Linear(2,2)\n",
    "        self.hidden_layer1 = nn.Linear(2,2)\n",
    "        self.output_layer = nn.Linear(2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.input(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = ANN()\n",
    "\n",
    "x_vec = torch.tensor( [0.1,0.2] )\n",
    "target = torch.tensor([1.0, 2])\n",
    "\n",
    "# ----- Forward Propagation -----\n",
    "output = model(x_vec)\n",
    "\n",
    "# ----- Compute Loss -----\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# ----- Backpropagation -----\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "optimizer.zero_grad() ## remove old gradients\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machinelearning)",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
