{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef77b431",
   "metadata": {},
   "source": [
    "![\"question\"](question.jpg)\n",
    "\n",
    "\n",
    "\n",
    "between layer 1 and 2:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  b_{10} & w_{11} & w_{21} \\\\\n",
    "  b_{20} & w_{12} & w_{22} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where b = bias and w = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb473e93",
   "metadata": {},
   "source": [
    "$x = [0.1,0.2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18bd8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f482b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.1,0.2],\n",
    "      [0.3,0.8]]) # x1, x2\n",
    "\n",
    "# x = [[0.1,0.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e67c6",
   "metadata": {},
   "source": [
    "input is columns, output is rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4efc0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(num_nodes_input_layer, num_nodes_output_layer):\n",
    "    \"\"\"Dimensions: output is rows, input is columns.\"\"\"\n",
    "    seed(1)\n",
    "    weights = np.random.rand(num_nodes_output_layer, num_nodes_input_layer + 1)\n",
    "    # biases = np.ones((weights.shape[0], 1))\n",
    "\n",
    "    # Stack the original array and the column of ones\n",
    "    # return np.column_stack((biases, weights ))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a7d05eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83057883, 0.44581712, 0.35657956],\n",
       "       [0.09503414, 0.49783754, 0.6120248 ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## define the layers\n",
    "seed(1)\n",
    "input = Linear(2,2)\n",
    "seed(1)\n",
    "hidden_layer1 = Linear(2,2)\n",
    "seed(1)\n",
    "output_layer = Linear(2,2)\n",
    "\n",
    "hidden_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b4593819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return np.array([1/(  1 + np.exp( -linear_combination )  ) for linear_combination in  z])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ae8ad",
   "metadata": {},
   "source": [
    "# Forward propogation to make predictions\n",
    "\n",
    "![\"how to apply the weights\"](apply_weights.png)\n",
    "\n",
    "$$ output = \n",
    "\\begin{bmatrix}\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:1 & Neuron\\:2\\:applied\\:to\\:sample\\:1 \\\\\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:2 & Neuron\\:2\\:applied\\:to\\:sample\\:2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8a5fc97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward(inputs, layer):\n",
    "    ## linear combination w0x0 + w1x1 + w2x2\n",
    "    linear_combinations = []\n",
    "\n",
    "    inputs = np.c_[ np.ones(x.shape[1]), x  ]    ## add bias to the inputs\n",
    "    # print(inputs)\n",
    "    # print(layer)\n",
    "\n",
    "    linear_combinations = np.matmul(inputs, layer.T)\n",
    "    \n",
    "    # print(linear_combinations)\n",
    "\n",
    "    sigmoids = 1/(  1 + np.exp( -linear_combinations ) )## compute a\n",
    "    # print(sigmoids)\n",
    "\n",
    "    return sigmoids, linear_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89817a22",
   "metadata": {},
   "source": [
    "# Predict the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(output_logits):\n",
    "    \"\"\"Compute the probability of a sample belonging to class 1 or 2\"\"\"\n",
    "    probability = np.exp(output_logits.T)/np.sum(np.exp(output_logits), axis = 1)\n",
    "\n",
    "    # print(np.exp(output_logits) ,np.sum(np.exp(output_logits), axis = 1))\n",
    "\n",
    "    return probability.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f13c3bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoids2, a2 = compute_forward(x, input)\n",
    "\n",
    "# sigmoids3, a3 = compute_forward(a2, hidden_layer1)\n",
    "\n",
    "otuput_sigmoids, a_output = compute_forward(a2, hidden_layer1)\n",
    "\n",
    "\n",
    "# otuput_sigmoids, a_output = compute_forward(a3, output_layer)\n",
    "\n",
    "probability = softmax(a_output)\n",
    "# signmoids, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64766b",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "$$\n",
    " J(θ)= - \\frac{1}{m} {\\sum^m_{i=1}\\sum^k_{k=1} [y^{(i)}_kln(\\frac{1}{1+e^{xθ^T}})_k + (1-y^{(i)}_k)ln(1-(\\frac{1}{1+e^{xθ^T}})_k)]} \n",
    "$$\n",
    "$$\n",
    " = \\frac{1}{m} {\\sum^m_{i=1}\\sum^k_{k=1} [y^{(i)}_kln(a^{(L)}_k + (1-y^{(i)}_k)ln(1-a^{(L)}_k)]}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "$$  \n",
    "output\\_sigmoids = \n",
    "\\begin{bmatrix}\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:1 & Neuron\\:2\\:applied\\:to\\:sample\\:1 \\\\\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:2 & Neuron\\:2\\:applied\\:to\\:sample\\:2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f3ad834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(output_sigmoids, target):\n",
    "    \n",
    "    ## m is number of samples\n",
    "    ## k is total number of output units\n",
    "\n",
    "    log_sigmoids = np.log(output_sigmoids)\n",
    "    y_times_log_sigmoids = np.matmul(log_sigmoids, target.T)\n",
    "    \n",
    "    one_minus_log_sigmoids = 1 - np.log(otuput_sigmoids)\n",
    "    one_minus_y_times_log_sigmoids = np.matmul(one_minus_log_sigmoids, target.T)\n",
    "    return (y_times_log_sigmoids + one_minus_y_times_log_sigmoids)/len(target)\n",
    "    # for sample in target:\n",
    "    #     for row in output_sigmoids:\n",
    "            \n",
    "    #         print(row, sample)\n",
    "    #     print(\"_____________\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a7262e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([1,0])\n",
    "binary_cross_entropy_loss(otuput_sigmoids, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7755d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_backpropogation(output_sigmoids):\n",
    "    \"\"\"\"\"\"\n",
    "    partial_z_partial_theta = output_sigmoids\n",
    "    partial_a_partial_z = output_sigmoids * (1-output_sigmoids)\n",
    "    partial_j_partial_a = () \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457fdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13692008, 0.4647963 ],\n",
       "       [0.65395656, 0.00602726]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(inputs):\n",
    "    inputs = in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c12b3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a4310957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.input = nn.Linear(2,2)\n",
    "        self.hidden_layer1 = nn.Linear(2,2)\n",
    "        self.output_layer = nn.Linear(2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = ANN()\n",
    "\n",
    "x_vec = torch.tensor( [0.1,0.2] )\n",
    "target = torch.tensor([1.0, 2])\n",
    "\n",
    "# ----- Forward Propagation -----\n",
    "output = model(x_vec)\n",
    "\n",
    "# ----- Compute Loss -----\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# ----- Backpropagation -----\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "optimizer.zero_grad() ## remove old gradients\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machinelearning)",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
