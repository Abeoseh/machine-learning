{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef77b431",
   "metadata": {},
   "source": [
    "![\"question\"](question.jpg)\n",
    "\n",
    "\n",
    "\n",
    "between layer 1 and 2:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  b_{10} & w_{11} & w_{21} \\\\\n",
    "  b_{20} & w_{12} & w_{22} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where b = bias and w = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb473e93",
   "metadata": {},
   "source": [
    "$x = [0.1,0.2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bd8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from random import seed\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8eac67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100, 4))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_iris()\n",
    "target = data.target[(data.target == 0) | (data.target == 1)]\n",
    "data = data.data[(data.target == 0) | (data.target == 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e67c6",
   "metadata": {},
   "source": [
    "input is columns, output is rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efc0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(num_nodes_input_layer, num_nodes_output_layer):\n",
    "    \"\"\"Dimensions: output is rows, input is columns.\"\"\"\n",
    "    weights = np.random.rand(num_nodes_output_layer, num_nodes_input_layer + 1) #* 0.01\n",
    "    # biases = np.ones((weights.shape[0], 1))\n",
    "\n",
    "    # Stack the original array and the column of ones\n",
    "    # return np.column_stack((biases, weights ))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ae8ad",
   "metadata": {},
   "source": [
    "# Forward propogation to make predictions\n",
    "\n",
    "![\"how to apply the weights\"](apply_weights.png)\n",
    "\n",
    "$$ output = \n",
    "\\begin{bmatrix}\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:1 & Neuron\\:2\\:applied\\:to\\:sample\\:1 \\\\\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:2 & Neuron\\:2\\:applied\\:to\\:sample\\:2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5fc97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward(inputs, layer):\n",
    "    ## linear combination w0x0 + w1x1 + w2x2\n",
    "    linear_combinations = []\n",
    "    # print(inputs, inputs.shape)\n",
    "    # print(np.ones(inputs.shape[1]))\n",
    "    inputs = np.c_[ np.ones(inputs.shape[0]), inputs  ]    ## add bias to the inputs\n",
    "    # print(inputs)\n",
    "    # print(layer)\n",
    "\n",
    "    linear_combinations = np.matmul(inputs, layer.T) ## logits or z\n",
    "    \n",
    "    # print(linear_combinations)\n",
    "\n",
    "    sigmoids = 1/(  1 + np.exp( -linear_combinations ) ) ## compute a\n",
    "    # print(sigmoids)\n",
    "\n",
    "    return linear_combinations, sigmoids\n",
    "\n",
    "\n",
    "# def sigmoid(inputs, layer):\n",
    "\n",
    "#     linear_combinations = np.matmul(inputs, layer.T) ## logits\n",
    "#     return 1/(  1 + np.exp( -linear_combinations ) ) ## compute a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64766b",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "*m is number of samples*\n",
    "\n",
    "*k is total number of output units*\n",
    "$$\n",
    " J(θ)= - \\frac{1}{m} {\\sum^m_{i=1}\\sum^k_{k=1} [y^{(i)}_kln(\\frac{1}{1+e^{xθ^T}})_k + (1-y^{(i)}_k)ln(1-(\\frac{1}{1+e^{xθ^T}})_k)]} \n",
    "$$\n",
    "$$\n",
    " = \\frac{1}{m} {\\sum^m_{i=1}\\sum^k_{k=1} [y^{(i)}_kln(a^{(L)}_k + (1-y^{(i)}_k)ln(1-a^{(L)}_k)]}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "$$  \n",
    "output\\_sigmoids = \n",
    "\\begin{bmatrix}\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:1 & Neuron\\:2\\:applied\\:to\\:sample\\:1 \\\\\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:2 & Neuron\\:2\\:applied\\:to\\:sample\\:2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ad834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(output_sigmoids, target):\n",
    "    \n",
    "    ## m is number of samples\n",
    "    ## k is total number of output units\n",
    "\n",
    "    target = target.reshape(-1,1)\n",
    "    log_sigmoids = np.log(output_sigmoids)\n",
    "    y_times_log_sigmoids = np.matmul(log_sigmoids, target.T)\n",
    "    # print(y_times_log_sigmoids)\n",
    "\n",
    "    one_minus_log_sigmoids = 1 - np.log(output_sigmoids)\n",
    "    one_minus_y_times_log_sigmoids = np.matmul(one_minus_log_sigmoids, (1 - target).T)\n",
    "    # print(one_minus_y_times_log_sigmoids)\n",
    "    \n",
    "    return sum(sum((y_times_log_sigmoids + one_minus_y_times_log_sigmoids)))/len(target) ## sum across the output units and sum across the samples\n",
    "    # for sample in target:\n",
    "    #     for row in output_sigmoids:\n",
    "            \n",
    "    #         print(row, sample)\n",
    "    #     print(\"_____________\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64301d0d",
   "metadata": {},
   "source": [
    "# Backpropogation\n",
    "\n",
    "want: $\\frac{∂J(θ)}{∂θ^{(l)}_{ij}}$ for $l=1,2$\n",
    "\n",
    "for each\n",
    "\n",
    "$$δ^{(L)} = \\begin{bmatrix}\n",
    "  a^{(L)}_{1} - y_{11} \\\\\n",
    "  a^{(L)}_{2} - y_{12} \\\\\n",
    "  a^{(L)}_1 - y_{21} \\\\ \n",
    "  a^{(L)}_1 - y_{22} \\\\ \n",
    "\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$y_{11}$ is sample $1$, for target $1 $\n",
    "\n",
    "$$δ^{(L)} * \\begin{bmatrix} a^{(L-1)}_{0}  a^{(L-1)}_{1} a^{(L-1)}_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "  \\frac{∂J(θ)}{∂θ^{(L-1)}_{10}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{11}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{12}} \\\\\n",
    "  \\\\\n",
    "  \\frac{∂J(θ)}{∂θ^{(L-1)}_{20}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{21}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{22}} \\\\\n",
    "\n",
    "\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "slides 37, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "b39a4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.1,0.2],\n",
    "            [0.3,0.8], \n",
    "            [0.3,0.8]]) # x1, x2\n",
    "\n",
    "y = np.array([[1,0],[1,1],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfee82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the layers\n",
    "# seed(1)\n",
    "#input = Linear(3,2)\n",
    "seed(1)\n",
    "hidden_layer1 = Linear(2,2)\n",
    "seed(1)\n",
    "output_layer = Linear(2,1)\n",
    "\n",
    "# hidden_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "7f85e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoids2, a2 = compute_forward(x, input)\n",
    "\n",
    "# sigmoids3, a3 = compute_forward(a2, hidden_layer1)\n",
    "\n",
    "z2, sigmoids2 = compute_forward(x, hidden_layer1)\n",
    "\n",
    "z_output, output_sigmoids = compute_forward(z2, output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "2679f711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6995779071347037"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cross_entropy_loss(output_sigmoids, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d308077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_backpropogation(sigmoids: list, layers: list, y, x):\n",
    "    \"\"\"\"\"\"\n",
    "    # print(\"layers before reversing\")\n",
    "    # print(layers)\n",
    "    # layers = [hidden_layer1, output_layer]\n",
    "\n",
    "    layers.reverse()\n",
    "    # print(\"layers after reversing\")\n",
    "    # print(layers)\n",
    "    # sigmoids = [sigmoids2, output_sigmoids]\n",
    "\n",
    "    sigmoids.insert(0, x)     \n",
    "    print(\"sigmoids before reversing\")\n",
    "    print(sigmoids)\n",
    "    sigmoids.reverse()\n",
    "    # print(\"sigmoids after reversing\")\n",
    "    # print(sigmoids)\n",
    "    gradients = []\n",
    "\n",
    "    y = y.reshape(-1,1)\n",
    "    # print(y)\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        if i == 0:\n",
    "\n",
    "            delta = (sigmoids[i] - y)\n",
    "            # print(delta.shape)\n",
    "            previous_sigmoids = np.c_[ np.ones(sigmoids[i+1].shape[0]), sigmoids[i+1]  ]    ## add bias to the inputs\n",
    "            # print(\"sigmoids for the last layer\")\n",
    "            # print(sigmoids[i])\n",
    "            # # print(\"__ __ __ __\")\n",
    "            # print(\"sigmoids for the previous layer\")\n",
    "            # print(previous_sigmoids)\n",
    "\n",
    "            ## delta has shape (samples x nodes)\n",
    "            ## sigmoids has shape (samples x activations + 1)\n",
    "            ## ex delta = sx2 (s = samples & nodes = 2 )\n",
    "            ##    sigmoids = sx3 (s = samples & there are 2 sigmoids and 1 to multiple the bias by)\n",
    "            gradients.append(np.matmul(delta.T, previous_sigmoids))\n",
    "\n",
    "\n",
    "        else:\n",
    "            ## the gradients accumulate across the samples so I may need to do\n",
    "            ## this for each sample then add all the gradients together. \n",
    "            # print(\"_______________________________\")\n",
    "            ## after the first delta, every previous delta is\n",
    "            ## (delta * (weights from the previous layer)) * sigmoids or x\n",
    "            delta = np.matmul(delta, layers[i-1][:,1:]) * sigmoids[i] * (1- sigmoids[i])\n",
    "            previous_sigmoids = np.c_[ np.ones(sigmoids[i+1].shape[0]), sigmoids[i+1]  ]    ## add bias to the inputs\n",
    "            # print(previous_sigmoids)\n",
    "            gradients.append(np.matmul(delta.T, previous_sigmoids))\n",
    "\n",
    "\n",
    "    # print(gradients)\n",
    "    return gradients\n",
    "\n",
    "def step(weights: list, gradients: list, lr = 0.01):\n",
    "    updated_weights = []\n",
    "    gradients.reverse()\n",
    "    for weight, gradient in zip(weights, gradients): ## so the first gradient is the input layers graidents\n",
    "        yield weight - (gradient * lr)\n",
    "        # weight = weight + (gradient * lr)\n",
    "        # updated_weights.append(weight)\n",
    "\n",
    "    # return updated_weights\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d56983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "def compute_backpropogation(sigmoids: list, layers: list, y: np.ndarray, x: np.ndarray):\n",
    "    \"\"\"\n",
    "    sigmoids: [A1, A2, ..., A_L]  (activations for each hidden and output layer, no bias columns)\n",
    "    layers:   [W1, W2, ..., W_L]  (Wk shape: (nodes_out_k, nodes_in_k + 1))\n",
    "    x:        input matrix (N, input_dim)\n",
    "    y:        target (N, 1)\n",
    "    returns:  gradients list in same order as layers: [dW1, dW2, ..., dW_L]\n",
    "    \"\"\"\n",
    "\n",
    "    # Make local copies — do not reverse/mutate the originals.\n",
    "    W = [w.copy() for w in layers]\n",
    "    A = [a.copy() for a in sigmoids]\n",
    "\n",
    "    # Build S = [X, A1, A2, ..., A_L]\n",
    "    S = [x] + A\n",
    "    N = x.shape[0]\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    L = len(W)   # number of layers\n",
    "\n",
    "    grads = [None] * L\n",
    "\n",
    "    # ---------- OUTPUT LAYER ----------\n",
    "    A_L = S[-1]             # last activation (N, out_dim)\n",
    "    A_prev = S[-2]          # activation of previous layer (N, prev_dim)\n",
    "    # delta for output (sigmoid + BCE): dZ = A_L - y\n",
    "    delta = (A_L - y)       # (N, out_dim)\n",
    "\n",
    "    # gradient dW_L = delta^T @ [1, A_prev]\n",
    "    A_prev_bias = np.c_[np.ones(N), A_prev]          # (N, prev_dim+1)\n",
    "    dW_L = delta.T @ A_prev_bias                     # (out_dim, prev_dim+1)\n",
    "    grads[-1] = dW_L\n",
    "\n",
    "    # ---------- HIDDEN LAYERS (iterate backwards) ----------\n",
    "    # iterate layer index from L-1 .. 0 (0-based)\n",
    "    for idx in range(L-2, -1, -1):\n",
    "        W_next = W[idx+1]            # next layer weights (out_next, in_next+1)\n",
    "        # remove bias column of next layer to propagate delta\n",
    "        W_next_no_bias = W_next[:, 1:]   # (out_next, in_next)  (note: out_next corresponds to current layer nodes)\n",
    "\n",
    "        # delta shape: (N, out_next). Multiply by W_next_no_bias (out_next, in_curr) -> (N, in_curr)\n",
    "        delta = (delta @ W_next_no_bias) * ( S[idx+1] * (1.0 - S[idx+1]) )  # (N, in_curr) elementwise\n",
    "        # previous activation (for gradient) is S[idx] (X for idx==0)\n",
    "        A_prev = S[idx]   # (N, in_curr_prev)\n",
    "        A_prev_bias = np.c_[np.ones(N), A_prev]   # (N, in_prev+1)\n",
    "        dW = delta.T @ A_prev_bias               # (in_curr, in_prev+1)  but note sizes align with W[idx] shape\n",
    "        grads[idx] = dW\n",
    "\n",
    "    # grads is [dW1, dW2, ..., dW_L] which matches layers order\n",
    "    return grads\n",
    "\n",
    "def step(weights: list, gradients: list, lr = 0.001):\n",
    "    \"\"\"\n",
    "    Non-generator updater. Returns updated weights list in same order.\n",
    "    \"\"\"\n",
    "    updated = []\n",
    "    # gradients already returned in the same order as weights above\n",
    "    for W, g in zip(weights, gradients):\n",
    "        updated.append(W + lr * g)\n",
    "    return updated\n",
    "\n",
    "# ------------------- Debugging helpers -------------------\n",
    "def debug_print_state(epoch, z_list, sigmoids_list, grads, weights_before, weights_after):\n",
    "    print(f\"--- epoch {epoch} ---\")\n",
    "    for i, (z, a) in enumerate(zip(z_list, sigmoids_list), 1):\n",
    "        print(f\"Layer {i}: z mean={np.mean(z):.4f}, z std={np.std(z):.4f}, a mean={np.mean(a):.4f}, a std={np.std(a):.4f}\")\n",
    "    for i, g in enumerate(grads, 1):\n",
    "        print(f\"grad L{i} norm: {np.linalg.norm(g):.6f}\")\n",
    "    for i, (wb, wa) in enumerate(zip(weights_before, weights_after), 1):\n",
    "        print(f\"W{i} change norm: {np.linalg.norm(wa - wb):.8f}\")\n",
    "\n",
    "# ------------------- Usage notes -------------------\n",
    "# 1) Call compute_backpropogation with your sigmoids in forward order [A1, A2, ...]\n",
    "# 2) Use step(...) to update all weights and reassign them (no generator).\n",
    "# 3) Use debug_print_state to inspect outputs/gradients/weight changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66e709e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.64065094, 0.75245143],\n",
      "       [0.532267  , 0.63693633],\n",
      "       [0.64065094, 0.75245143]]), array([[0.83696988],\n",
      "       [0.8132223 ],\n",
      "       [0.83696988]])]\n",
      "----------------------epoch: 0-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.1875617179683853\n",
      "[[0.83696988]\n",
      " [0.8132223 ]\n",
      " [0.83696988]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.63798371, 0.75020661],\n",
      "       [0.530096  , 0.63487721],\n",
      "       [0.63798371, 0.75020661]]), array([[0.81519327],\n",
      "       [0.79111218],\n",
      "       [0.81519327]])]\n",
      "----------------------epoch: 1-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.214325200993765\n",
      "[[0.81519327]\n",
      " [0.79111218]\n",
      " [0.81519327]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.6356089 , 0.74818519],\n",
      "       [0.52820173, 0.63306839],\n",
      "       [0.6356089 , 0.74818519]]), array([[0.7924489 ],\n",
      "       [0.76834556],\n",
      "       [0.7924489 ]])]\n",
      "----------------------epoch: 2-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.2429234037628922\n",
      "[[0.7924489 ]\n",
      " [0.76834556]\n",
      " [0.7924489 ]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.63350998, 0.74637824],\n",
      "       [0.5265649 , 0.63149589],\n",
      "       [0.63350998, 0.74637824]]), array([[0.76903008],\n",
      "       [0.74520585],\n",
      "       [0.76903008]])]\n",
      "----------------------epoch: 3-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.273115062357993\n",
      "[[0.76903008]\n",
      " [0.74520585]\n",
      " [0.76903008]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.63166818, 0.74477431],\n",
      "       [0.52516442, 0.63014326],\n",
      "       [0.63166818, 0.74477431]]), array([[0.74524339],\n",
      "       [0.72197559],\n",
      "       [0.74524339]])]\n",
      "----------------------epoch: 4-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.3046175985173925\n",
      "[[0.74524339]\n",
      " [0.72197559]\n",
      " [0.74524339]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.63006322, 0.74336002],\n",
      "       [0.52397818, 0.62899235],\n",
      "       [0.63006322, 0.74336002]]), array([[0.72139048],\n",
      "       [0.69892156],\n",
      "       [0.72139048]])]\n",
      "----------------------epoch: 5-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.3371220623526843\n",
      "[[0.72139048]\n",
      " [0.69892156]\n",
      " [0.72139048]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62867402, 0.7421207 ],\n",
      "       [0.52298369, 0.62802397],\n",
      "       [0.62867402, 0.7421207 ]]), array([[0.69775211],\n",
      "       [0.67628284],\n",
      "       [0.69775211]])]\n",
      "----------------------epoch: 6-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.3703088807707497\n",
      "[[0.69775211]\n",
      " [0.67628284]\n",
      " [0.69775211]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62747935, 0.74104095],\n",
      "       [0.52215875, 0.62721865],\n",
      "       [0.62747935, 0.74104095]]), array([[0.67457606],\n",
      "       [0.65426267],\n",
      "       [0.67457606]])]\n",
      "----------------------epoch: 7-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.403862682493823\n",
      "[[0.67457606]\n",
      " [0.65426267]\n",
      " [0.67457606]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62645848, 0.74010527],\n",
      "       [0.52148197, 0.62655722],\n",
      "       [0.62645848, 0.74010527]]), array([[0.65206949],\n",
      "       [0.6330244 ],\n",
      "       [0.65206949]])]\n",
      "----------------------epoch: 8-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.4374848632876946\n",
      "[[0.65206949]\n",
      " [0.6330244 ]\n",
      " [0.65206949]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62559157, 0.73929844],\n",
      "       [0.52093324, 0.62602128],\n",
      "       [0.62559157, 0.73929844]]), array([[0.63039585],\n",
      "       [0.61269093],\n",
      "       [0.63039585]])]\n",
      "----------------------epoch: 9-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.4709031046432473\n",
      "[[0.63039585]\n",
      " [0.61269093]\n",
      " [0.63039585]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62486003, 0.73860592],\n",
      "       [0.52049395, 0.62559355],\n",
      "       [0.62486003, 0.73860592]]), array([[0.60967544],\n",
      "       [0.59334707],\n",
      "       [0.60967544]])]\n",
      "----------------------epoch: 10-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.5038776122478534\n",
      "[[0.60967544]\n",
      " [0.59334707]\n",
      " [0.60967544]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62424674, 0.7380141 ],\n",
      "       [0.5201472 , 0.62525815],\n",
      "       [0.62424674, 0.7380141 ]]), array([[0.58998877],\n",
      "       [0.57504367],\n",
      "       [0.58998877]])]\n",
      "----------------------epoch: 11-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.536204284998034\n",
      "[[0.58998877]\n",
      " [0.57504367]\n",
      " [0.58998877]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62373615, 0.73751041],\n",
      "       [0.51987784, 0.6250007 ],\n",
      "       [0.62373615, 0.73751041]]), array([[0.57138157],\n",
      "       [0.55780277],\n",
      "       [0.57138157]])]\n",
      "----------------------epoch: 12-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.567715309682418\n",
      "[[0.57138157]\n",
      " [0.55780277]\n",
      " [0.57138157]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62331429, 0.73708346],\n",
      "       [0.51967249, 0.62480834],\n",
      "       [0.62331429, 0.73708346]]), array([[0.55387057],\n",
      "       [0.54162309],\n",
      "       [0.55387057]])]\n",
      "----------------------epoch: 13-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.5982778028853173\n",
      "[[0.55387057]\n",
      " [0.54162309]\n",
      " [0.55387057]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62296877, 0.73672302],\n",
      "       [0.51951941, 0.62466976],\n",
      "       [0.62296877, 0.73672302]]), array([[0.53744939],\n",
      "       [0.52648523],\n",
      "       [0.53744939]])]\n",
      "----------------------epoch: 14-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.627791124685671\n",
      "[[0.53744939]\n",
      " [0.52648523]\n",
      " [0.53744939]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62268866, 0.73641999],\n",
      "       [0.51940846, 0.62457503],\n",
      "       [0.62268866, 0.73641999]]), array([[0.52209398],\n",
      "       [0.51235635],\n",
      "       [0.52209398]])]\n",
      "----------------------epoch: 15-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.656183413780736\n",
      "[[0.52209398]\n",
      " [0.51235635]\n",
      " [0.52209398]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62246442, 0.73616634],\n",
      "       [0.51933087, 0.62451558],\n",
      "       [0.62246442, 0.73616634]]), array([[0.50776745],\n",
      "       [0.49919428],\n",
      "       [0.50776745]])]\n",
      "----------------------epoch: 16-----------------------\n",
      "predictions:\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.683407782050207\n",
      "[[0.50776745]\n",
      " [0.49919428]\n",
      " [0.50776745]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62228772, 0.73595503],\n",
      "       [0.5192792 , 0.62448406],\n",
      "       [0.62228772, 0.73595503]]), array([[0.49442413],\n",
      "       [0.48695074],\n",
      "       [0.49442413]])]\n",
      "----------------------epoch: 17-----------------------\n",
      "predictions:\n",
      "[[0]\n",
      " [0]\n",
      " [0]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.7094384884842015\n",
      "[[0.49442413]\n",
      " [0.48695074]\n",
      " [0.49442413]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62215137, 0.73577992],\n",
      "       [0.51924711, 0.62447416],\n",
      "       [0.62215137, 0.73577992]]), array([[0.48201282],\n",
      "       [0.475574  ],\n",
      "       [0.48201282]])]\n",
      "----------------------epoch: 18-----------------------\n",
      "predictions:\n",
      "[[0]\n",
      " [0]\n",
      " [0]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.7342673059483182\n",
      "[[0.48201282]\n",
      " [0.475574  ]\n",
      " [0.48201282]]\n",
      "sigmoids before reversing\n",
      "[array([[ 0.1 ,  0.2 ],\n",
      "       [-0.04, -0.6 ],\n",
      "       [ 0.1 ,  0.2 ]]), array([[0.62204916, 0.73563568],\n",
      "       [0.51922926, 0.62448056],\n",
      "       [0.62204916, 0.73563568]]), array([[0.47047939],\n",
      "       [0.46501093],\n",
      "       [0.47047939]])]\n",
      "----------------------epoch: 19-----------------------\n",
      "predictions:\n",
      "[[0]\n",
      " [0]\n",
      " [0]]\n",
      "Actual:\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n",
      "error: 2.75790020774896\n",
      "[[0.47047939]\n",
      " [0.46501093]\n",
      " [0.47047939]]\n"
     ]
    }
   ],
   "source": [
    "### put it all together:\n",
    "EPOCHS = 20\n",
    "x = np.array([[0.1,0.2],\n",
    "              [-.04,-0.6], \n",
    "              [0.1,0.2]]) # x1, x2\n",
    "\n",
    "# # y = np.array([[1,0],[1,1],[0,1]])\n",
    "y = np.array([[0],[1],[0]])\n",
    "\n",
    "\n",
    "hidden_layer1 = Linear(2,2) \n",
    "\n",
    "output_layer = Linear(2,1) \n",
    "\n",
    "# print(hidden_layer1, output_layer)\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    z2, sigmoids2 = compute_forward(x, hidden_layer1)\n",
    "\n",
    "    z_output, output_sigmoids = compute_forward(sigmoids2, output_layer)\n",
    "\n",
    "    loss = binary_cross_entropy_loss(output_sigmoids, y)\n",
    "\n",
    "\n",
    "    # l2_gradients = compute_backpropogation(output_sigmoids, y, sigmoids2)\n",
    "\n",
    "\n",
    "    # l1_gradients = compute_backpropogation(sigmoids2, y, x)\n",
    "\n",
    "    # output_layer1 = step(hidden_layer1, l1_gradients, 0.1)\n",
    "\n",
    "    gradients = compute_backpropogation(sigmoids = [sigmoids2, output_sigmoids], layers = [hidden_layer1, output_layer], y=y, x=x)\n",
    "    hidden_layer1, output_layer = step(weights = [hidden_layer1, output_layer], gradients = gradients, lr = 0.05)\n",
    "    # print(gradients)\n",
    "    \n",
    "\n",
    "    print(f\"----------------------epoch: {epoch}-----------------------\")\n",
    "    print(f\"predictions:\\n{(output_sigmoids > 0.5).astype(int)}\\nActual:\\n{y}\")\n",
    "    print(f\"error: {loss}\")\n",
    "    print(output_sigmoids)\n",
    "    \n",
    "\n",
    "    # update_all_layers([hidden_layer1, output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "806c92ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37134068, 0.2542515 , 0.29879574],\n",
       "       [0.34602962, 0.84902082, 0.0763705 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9779202c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69629129],\n",
       "       [0.73525819],\n",
       "       [0.69629129]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sigmoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457fdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13692008, 0.4647963 ],\n",
       "       [0.65395656, 0.00602726]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(inputs):\n",
    "    inputs = in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c12b3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4310957",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m tensor_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(target)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# ----- Forward Propagation -----\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# ----- Compute Loss -----\u001b[39;00m\n\u001b[0;32m     28\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n",
      "File \u001b[1;32mc:\\Users\\brean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[49], line 13\u001b[0m, in \u001b[0;36mANN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# x = self.input(x)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "File \u001b[1;32mc:\\Users\\brean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\brean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\brean\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # self.input = nn.Linear(2,2)\n",
    "        self.hidden_layer1 = nn.Linear(4,2)\n",
    "        self.output_layer = nn.Linear(2,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.input(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = ANN()\n",
    "\n",
    "x_vec = torch.tensor( data )\n",
    "tensor_target = torch.tensor(target)\n",
    "\n",
    "# ----- Forward Propagation -----\n",
    "output = model(x_vec)\n",
    "\n",
    "# ----- Compute Loss -----\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(output, tensor_target)\n",
    "\n",
    "# ----- Backpropagation -----\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "optimizer.zero_grad() ## remove old gradients\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machinelearning)",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
