{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef77b431",
   "metadata": {},
   "source": [
    "![\"question\"](question.jpg)\n",
    "\n",
    "\n",
    "\n",
    "between layer 1 and 2:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  b_{10} & w_{11} & w_{21} \\\\\n",
    "  b_{20} & w_{12} & w_{22} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where b = bias and w = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb473e93",
   "metadata": {},
   "source": [
    "$x = [0.1,0.2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18bd8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e67c6",
   "metadata": {},
   "source": [
    "input is columns, output is rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4efc0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(num_nodes_input_layer, num_nodes_output_layer):\n",
    "    \"\"\"Dimensions: output is rows, input is columns.\"\"\"\n",
    "    seed(1)\n",
    "    weights = np.random.rand(num_nodes_output_layer, num_nodes_input_layer + 1)\n",
    "    # biases = np.ones((weights.shape[0], 1))\n",
    "\n",
    "    # Stack the original array and the column of ones\n",
    "    # return np.column_stack((biases, weights ))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ae8ad",
   "metadata": {},
   "source": [
    "# Forward propogation to make predictions\n",
    "\n",
    "![\"how to apply the weights\"](apply_weights.png)\n",
    "\n",
    "$$ output = \n",
    "\\begin{bmatrix}\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:1 & Neuron\\:2\\:applied\\:to\\:sample\\:1 \\\\\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:2 & Neuron\\:2\\:applied\\:to\\:sample\\:2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "8a5fc97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward(inputs, layer):\n",
    "    ## linear combination w0x0 + w1x1 + w2x2\n",
    "    linear_combinations = []\n",
    "    # print(inputs, inputs.shape)\n",
    "    # print(np.ones(inputs.shape[1]))\n",
    "    inputs = np.c_[ np.ones(inputs.shape[0]), inputs  ]    ## add bias to the inputs\n",
    "    # print(inputs)\n",
    "    # print(layer)\n",
    "\n",
    "    linear_combinations = np.matmul(inputs, layer.T) ## logits or z\n",
    "    \n",
    "    # print(linear_combinations)\n",
    "\n",
    "    sigmoids = 1/(  1 + np.exp( -linear_combinations ) ) ## compute a\n",
    "    # print(sigmoids)\n",
    "\n",
    "    return linear_combinations, sigmoids\n",
    "\n",
    "\n",
    "# def sigmoid(inputs, layer):\n",
    "\n",
    "#     linear_combinations = np.matmul(inputs, layer.T) ## logits\n",
    "#     return 1/(  1 + np.exp( -linear_combinations ) ) ## compute a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64766b",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "*m is number of samples*\n",
    "\n",
    "*k is total number of output units*\n",
    "$$\n",
    " J(θ)= - \\frac{1}{m} {\\sum^m_{i=1}\\sum^k_{k=1} [y^{(i)}_kln(\\frac{1}{1+e^{xθ^T}})_k + (1-y^{(i)}_k)ln(1-(\\frac{1}{1+e^{xθ^T}})_k)]} \n",
    "$$\n",
    "$$\n",
    " = \\frac{1}{m} {\\sum^m_{i=1}\\sum^k_{k=1} [y^{(i)}_kln(a^{(L)}_k + (1-y^{(i)}_k)ln(1-a^{(L)}_k)]}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "$$  \n",
    "output\\_sigmoids = \n",
    "\\begin{bmatrix}\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:1 & Neuron\\:2\\:applied\\:to\\:sample\\:1 \\\\\n",
    "  Neuron\\:1\\:applied\\:to\\:sample\\:2 & Neuron\\:2\\:applied\\:to\\:sample\\:2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "f3ad834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(output_sigmoids, target):\n",
    "    \n",
    "    ## m is number of samples\n",
    "    ## k is total number of output units\n",
    "\n",
    "    log_sigmoids = np.log(output_sigmoids)\n",
    "    y_times_log_sigmoids = np.matmul(log_sigmoids, target.T)\n",
    "    # print(y_times_log_sigmoids)\n",
    "\n",
    "    one_minus_log_sigmoids = 1 - np.log(output_sigmoids)\n",
    "    one_minus_y_times_log_sigmoids = np.matmul(one_minus_log_sigmoids, (1 - target).T)\n",
    "    # print(one_minus_y_times_log_sigmoids)\n",
    "    \n",
    "    return sum(sum((y_times_log_sigmoids + one_minus_y_times_log_sigmoids)))/len(target) ## sum across the output units and sum across the samples\n",
    "    # for sample in target:\n",
    "    #     for row in output_sigmoids:\n",
    "            \n",
    "    #         print(row, sample)\n",
    "    #     print(\"_____________\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64301d0d",
   "metadata": {},
   "source": [
    "# Backpropogation\n",
    "\n",
    "want: $\\frac{∂J(θ)}{∂θ^{(l)}_{ij}}$ for $l=1,2$\n",
    "\n",
    "for each\n",
    "\n",
    "$$δ^{(L)} = \\begin{bmatrix}\n",
    "  a^{(L)}_{1} - y_{11} \\\\\n",
    "  a^{(L)}_{2} - y_{12} \\\\\n",
    "  a^{(L)}_1 - y_{21} \\\\ \n",
    "  a^{(L)}_1 - y_{22} \\\\ \n",
    "\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$y_{11}$ is sample $1$, for target $1 $\n",
    "\n",
    "$$δ^{(L)} * \\begin{bmatrix} a^{(L-1)}_{0}  a^{(L-1)}_{1} a^{(L-1)}_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "  \\frac{∂J(θ)}{∂θ^{(L-1)}_{10}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{11}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{12}} \\\\\n",
    "  \\\\\n",
    "  \\frac{∂J(θ)}{∂θ^{(L-1)}_{20}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{21}} & \\frac{∂J(θ)}{∂θ^{(L-1)}_{22}} \\\\\n",
    "\n",
    "\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "slides 37, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "b39a4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.1,0.2],\n",
    "            [0.3,0.8], \n",
    "            [0.3,0.8]]) # x1, x2\n",
    "\n",
    "y = np.array([[1,0],[1,1],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "ecfee82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the layers\n",
    "# seed(1)\n",
    "#input = Linear(3,2)\n",
    "seed(1)\n",
    "hidden_layer1 = Linear(2,2)\n",
    "seed(1)\n",
    "output_layer = Linear(2,2)\n",
    "\n",
    "# hidden_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "7f85e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoids2, a2 = compute_forward(x, input)\n",
    "\n",
    "# sigmoids3, a3 = compute_forward(a2, hidden_layer1)\n",
    "\n",
    "z2, sigmoids2 = compute_forward(x, hidden_layer1)\n",
    "\n",
    "z_output, output_sigmoids = compute_forward(z2, output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "2679f711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6995779071347037"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cross_entropy_loss(output_sigmoids, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "7d308077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_backpropogation(sigmoids: list, layers: list, y, x):\n",
    "    \"\"\"\"\"\"\n",
    "    # layers = [hidden_layer1, output_layer]\n",
    "    layers.reverse()\n",
    "    # sigmoids = [sigmoids2, output_sigmoids]\n",
    "    sigmoids.insert(0, x)\n",
    "    sigmoids.reverse()\n",
    "    gradients = []\n",
    "\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        if i == 0:\n",
    "            delta = (sigmoids[i] - y)\n",
    "            # print(delta.shape)\n",
    "            previous_sigmoids = np.c_[ np.ones(sigmoids[i+1].shape[0]), sigmoids[i+1]  ]    ## add bias to the inputs\n",
    "            # print(sigmoids[i])\n",
    "            # print(\"__ __ __ __\")\n",
    "            # print(previous_sigmoids)\n",
    "\n",
    "            ## delta has shape (samples x nodes)\n",
    "            ## sigmoids has shape (samples x activations + 1)\n",
    "            ## ex delta = sx2 (s = samples & nodes = 2 )\n",
    "            ##    sigmoids = sx3 (s = samples & there are 2 sigmoids and 1 to multiple the bias by)\n",
    "            gradients.append(np.matmul(delta.T, previous_sigmoids))\n",
    "\n",
    "\n",
    "        else:\n",
    "            # print(\"_______________________________\")\n",
    "            ## after the first delta, every previous delta is\n",
    "            ## (delta * (weights from the previous layer)) * sigmoids or x\n",
    "            delta = np.matmul(delta, layers[i-1][:,1:]) * sigmoids[i] * (1- sigmoids[i])\n",
    "            previous_sigmoids = np.c_[ np.ones(sigmoids[i+1].shape[0]), sigmoids[i+1]  ]    ## add bias to the inputs\n",
    "            # print(previous_sigmoids)\n",
    "            gradients.append(np.matmul(delta.T, previous_sigmoids))\n",
    "\n",
    "\n",
    "    # print(gradients)\n",
    "    return gradients\n",
    "\n",
    "def step(weights: list, gradients: list, lr = 0.001):\n",
    "    gradients.reverse()\n",
    "    for weight, gradient in zip(weights, gradients): ## so the first gradient is the input layers graidents\n",
    "        yield weight - (gradient * lr)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "66e709e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37547358 0.50973342 0.25291365]\n",
      " [0.85255273 0.26147232 0.95869677]] [[0.75498399 0.16564308 0.35487226]\n",
      " [0.96016676 0.5314894  0.96910551]]\n",
      "----------------------epoch: 0-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77098296 0.90474293]\n",
      " [0.81435792 0.95230385]\n",
      " [0.81435792 0.95230385]]\n",
      "error: 5.736748502010941\n",
      "----------------------epoch: 1-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77094424 0.90468893]\n",
      " [0.81431488 0.95226689]\n",
      " [0.81431488 0.95226689]]\n",
      "error: 5.736950225164719\n",
      "----------------------epoch: 2-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.7709055  0.9046349 ]\n",
      " [0.81427181 0.95222989]\n",
      " [0.81427181 0.95222989]]\n",
      "error: 5.737152053950748\n",
      "----------------------epoch: 3-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77086675 0.90458083]\n",
      " [0.81422874 0.95219285]\n",
      " [0.81422874 0.95219285]]\n",
      "error: 5.7373539884308355\n",
      "----------------------epoch: 4-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77082798 0.90452671]\n",
      " [0.81418564 0.95215578]\n",
      " [0.81418564 0.95215578]]\n",
      "error: 5.737556028666808\n",
      "----------------------epoch: 5-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77078921 0.90447257]\n",
      " [0.81414253 0.95211867]\n",
      " [0.81414253 0.95211867]]\n",
      "error: 5.73775817472051\n",
      "----------------------epoch: 6-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77075042 0.90441838]\n",
      " [0.8140994  0.95208153]\n",
      " [0.8140994  0.95208153]]\n",
      "error: 5.737960426653806\n",
      "----------------------epoch: 7-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77071162 0.90436416]\n",
      " [0.81405625 0.95204436]\n",
      " [0.81405625 0.95204436]]\n",
      "error: 5.738162784528576\n",
      "----------------------epoch: 8-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77067281 0.90430989]\n",
      " [0.81401309 0.95200714]\n",
      " [0.81401309 0.95200714]]\n",
      "error: 5.738365248406724\n",
      "----------------------epoch: 9-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77063398 0.90425559]\n",
      " [0.81396991 0.9519699 ]\n",
      " [0.81396991 0.9519699 ]]\n",
      "error: 5.738567818350167\n",
      "----------------------epoch: 10-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77059514 0.90420125]\n",
      " [0.81392672 0.95193262]\n",
      " [0.81392672 0.95193262]]\n",
      "error: 5.738770494420844\n",
      "----------------------epoch: 11-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77055629 0.90414688]\n",
      " [0.81388351 0.9518953 ]\n",
      " [0.81388351 0.9518953 ]]\n",
      "error: 5.738973276680713\n",
      "----------------------epoch: 12-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77051743 0.90409246]\n",
      " [0.81384028 0.95185795]\n",
      " [0.81384028 0.95185795]]\n",
      "error: 5.7391761651917435\n",
      "----------------------epoch: 13-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77047855 0.90403801]\n",
      " [0.81379703 0.95182057]\n",
      " [0.81379703 0.95182057]]\n",
      "error: 5.7393791600159325\n",
      "----------------------epoch: 14-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77043967 0.90398352]\n",
      " [0.81375377 0.95178314]\n",
      " [0.81375377 0.95178314]]\n",
      "error: 5.73958226121529\n",
      "----------------------epoch: 15-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77040077 0.90392899]\n",
      " [0.81371049 0.95174569]\n",
      " [0.81371049 0.95174569]]\n",
      "error: 5.739785468851845\n",
      "----------------------epoch: 16-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77036186 0.90387442]\n",
      " [0.81366719 0.9517082 ]\n",
      " [0.81366719 0.9517082 ]]\n",
      "error: 5.739988782987645\n",
      "----------------------epoch: 17-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77032293 0.90381982]\n",
      " [0.81362388 0.95167067]\n",
      " [0.81362388 0.95167067]]\n",
      "error: 5.740192203684756\n",
      "----------------------epoch: 18-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77028399 0.90376517]\n",
      " [0.81358055 0.95163311]\n",
      " [0.81358055 0.95163311]]\n",
      "error: 5.74039573100526\n",
      "----------------------epoch: 19-----------------------\n",
      "predictions:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "Actual:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]], \n",
      " [[0.77024505 0.90371049]\n",
      " [0.8135372  0.95159551]\n",
      " [0.8135372  0.95159551]]\n",
      "error: 5.740599365011259\n"
     ]
    }
   ],
   "source": [
    "### put it all together:\n",
    "EPOCHS = 20\n",
    "x = np.array([[0.1,0.2],\n",
    "            [0.3,0.8], \n",
    "            [0.3,0.8]]) # x1, x2\n",
    "\n",
    "# y = np.array([[1,0],[1,1],[0,1]])\n",
    "y = np.array([[0,0],[0,1],[0,0]])\n",
    "\n",
    "\n",
    "seed(1)\n",
    "hidden_layer1 = Linear(2,2) \n",
    "seed(1)\n",
    "output_layer = Linear(2,2) \n",
    "\n",
    "print(hidden_layer1, output_layer)\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    z2, sigmoids2 = compute_forward(x, hidden_layer1)\n",
    "\n",
    "    z_output, output_sigmoids = compute_forward(z2, output_layer)\n",
    "\n",
    "    loss = binary_cross_entropy_loss(output_sigmoids, y)\n",
    "\n",
    "\n",
    "    # l2_gradients = compute_backpropogation(output_sigmoids, y, sigmoids2)\n",
    "\n",
    "\n",
    "    # l1_gradients = compute_backpropogation(sigmoids2, y, x)\n",
    "\n",
    "    # output_layer1 = step(hidden_layer1, l1_gradients, 0.1)\n",
    "\n",
    "    gradients = compute_backpropogation(sigmoids = [sigmoids2, output_sigmoids], layers = [hidden_layer1, output_layer], y=y, x=x)\n",
    "    hidden_layer1, output_layer1 = step(weights = [hidden_layer1, output_layer], gradients = gradients, lr = 0.001)\n",
    "    # print(gradients)\n",
    "    \n",
    "\n",
    "    print(f\"----------------------epoch: {epoch}-----------------------\")\n",
    "    print(f\"predictions:\\n{(output_sigmoids > 0.5).astype(int)}\\nActual:\\n{y}, \\n {output_sigmoids}\")\n",
    "    print(f\"error: {loss}\")\n",
    "    \n",
    "\n",
    "    # update_all_layers([hidden_layer1, output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457fdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13692008, 0.4647963 ],\n",
       "       [0.65395656, 0.00602726]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(inputs):\n",
    "    inputs = in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c12b3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "a4310957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # self.input = nn.Linear(2,2)\n",
    "        self.hidden_layer1 = nn.Linear(2,1)\n",
    "        self.output_layer = nn.Linear(1,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.input(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = ANN()\n",
    "\n",
    "x_vec = torch.tensor( [0.1,0.2] )\n",
    "target = torch.tensor([1.0, 2])\n",
    "\n",
    "# ----- Forward Propagation -----\n",
    "output = model(x_vec)\n",
    "\n",
    "# ----- Compute Loss -----\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# ----- Backpropagation -----\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "optimizer.zero_grad() ## remove old gradients\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machinelearning)",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
