{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d808aafd",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "(a) \n",
    "> (10 points) Do natural log transform of the PFOS variable in file\n",
    "pfas.csv and store the results as a new variable log PFOS in the\n",
    "data file. Standardize the variables x=[log PFOS, age, gender, BMI]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740fb93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brean\\AppData\\Local\\Temp\\ipykernel_4116\\1614590724.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as api\n",
    "import statsmodels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84aecb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoscale(df, variables_to_scale):\n",
    "    \"\"\"standardizes variables\"\"\"\n",
    "    for variable in variables_to_scale:\n",
    "        df[variable] = ( df[variable] - np.mean(df[variable]) ) / np.std(df[variable])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6b566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease\n",
      " [0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
      " 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
      " 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0\n",
      " 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1\n",
      " 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1\n",
      " 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1\n",
      " 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0\n",
      " 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1\n",
      " 0 0 1 0]\n",
      "     intercept|    age   |   gender     |  BMI    |    log_PFOS\n",
      " [[ 1.         -1.06321047 -1.06191317 -0.78453458  0.90375231]\n",
      " [ 1.          0.96375408 -1.06191317 -1.18985472 -0.99778188]\n",
      " [ 1.          0.31512542  0.94169658 -0.5470809  -0.24350772]\n",
      " ...\n",
      " [ 1.          0.396204    0.94169658  0.07829738 -0.24248557]\n",
      " [ 1.          0.15296826  0.94169658  0.8139823  -0.23195204]\n",
      " [ 1.          0.80159691 -1.06191317  0.33858428  0.29556023]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"pfas.csv\")\n",
    "df[\"log_PFOS\"] = np.log(df[\"PFOS\"])\n",
    "\n",
    "autoscale(df, [\"age\", \"gender\", \"BMI\", \"log_PFOS\"])\n",
    "\n",
    "#### Initalize variables\n",
    "Y = df[\"disease\"].to_numpy()\n",
    "X = df.loc[:,[\"age\", \"gender\", \"BMI\", \"log_PFOS\"]].to_numpy()\n",
    "## add intercept\n",
    "X_with_intercept = np.insert( X, 0, np.array([1]*X.shape[0]), axis = 1 )\n",
    "\n",
    "\n",
    "print(\"disease\\n\", Y)\n",
    "print(\"     intercept|    age   |   gender     |  BMI    |    log_PFOS\\n\",X_with_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e3534",
   "metadata": {},
   "source": [
    "#### Initalize variables using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f19145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease\n",
      " [0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
      " 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
      " 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0\n",
      " 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1\n",
      " 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1\n",
      " 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1\n",
      " 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0\n",
      " 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1\n",
      " 0 0 1 0]\n",
      "     intercept|    age   |   gender     |  BMI    |    log_PFOS\n",
      " [[ 1.         -1.06321047 -1.06191317 -0.78453458  0.90375231]\n",
      " [ 1.          0.96375408 -1.06191317 -1.18985472 -0.99778188]\n",
      " [ 1.          0.31512542  0.94169658 -0.5470809  -0.24350772]\n",
      " ...\n",
      " [ 1.          0.396204    0.94169658  0.07829738 -0.24248557]\n",
      " [ 1.          0.15296826  0.94169658  0.8139823  -0.23195204]\n",
      " [ 1.          0.80159691 -1.06191317  0.33858428  0.29556023]]\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"pfas.csv\")\n",
    "df2[\"log_PFOS\"] = np.log2(df2[\"PFOS\"])\n",
    "\n",
    "# StandardScaler().fit(df2[[\"log_PFOS\", \"age\", \"gender\", \"BMI\"]])\n",
    "X = StandardScaler().fit_transform(df2[[\"age\", \"gender\", \"BMI\", \"log_PFOS\"]])\n",
    "X_with_intercept2 = np.insert( X, 0, np.array([1]*X.shape[0]), axis = 1 )\n",
    "\n",
    "Y = df[\"disease\"].to_numpy()\n",
    "\n",
    "print(\"disease\\n\", Y)\n",
    "print(\"     intercept|    age   |   gender     |  BMI    |    log_PFOS\\n\",X_with_intercept2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5954c47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in (X_with_intercept == X_with_intercept2) ## standardscaler gives the same output as my version!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ccfa6",
   "metadata": {},
   "source": [
    "### Numerical Solution\n",
    "(b) \n",
    "> (35 points) Use y=disease and the standardized x=[PFOS, age,\n",
    "gender, BMI] to write and debug your own gradient descent algorithm for logistic regression. Your algorithm should export the\n",
    "learned parameters in the θ vector. Note that you can modify the\n",
    "gradient descent algorithm that you have written for the linear regression algorithm to achieve logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373675a3",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "\n",
    "$ g(B^Tx)= \\frac{1}{1+e^{-(B^Tx)}}$\n",
    "\n",
    "$B^Tx=B_0x_0+B_1x_1+...+B_nx_n$\n",
    "\n",
    "$x_0=1$\n",
    "\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    \\text{1 if} & B^Tx\\text{ ≥ 0} \\\\\n",
    "    \\text{0 if} & B^Tx\\text{< 0} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "$i$ is a row $j$ is a column\n",
    "\n",
    "$ J(B) = -\\frac{1}{m} \\sum_{i=1}^m [y_i ln(\\frac{1}{1+e^{-(B^Tx)}}) + (1-y_i)ln(\\frac{1}{1+e^{-(B^Tx)}})]$\n",
    "\n",
    "$ B_j := B_j-α(\\frac{∂J(B)}{∂B_j}) $\n",
    "\n",
    "$ \\frac{∂J(B)}{∂B_j} = \\frac{1}{m} \\sum [(\\frac{1}{1+e^{-(B^Tx)}}) - y_i]x_{j,i}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e1dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------epoch: 0------------------------------------\n",
      "\t\t\t\tcost: 0.6931471805599453\n",
      "b_dict: {'intercept': 0.003666666666666667, 'log_PFOS': 0.005614781879612744, 'age': 0.0071261720081734915, 'gender': 0.009237937363079844, 'BMI': 9.976846641748035e-05}\n",
      "------------------------------------epoch: 100------------------------------------\n",
      "\t\t\t\tcost: 0.6539572177431865\n",
      "b_dict: {'intercept': 0.13972505594387125, 'log_PFOS': 0.21388705051948634, 'age': 0.2507497017915419, 'gender': 0.3237468146185928, 'BMI': 0.023679711181707166}\n",
      "------------------------------------epoch: 200------------------------------------\n",
      "\t\t\t\tcost: 0.6522490102240123\n",
      "b_dict: {'intercept': 0.15402810805273662, 'log_PFOS': 0.23758336393223678, 'age': 0.2717541566678402, 'gender': 0.34957447968201444, 'BMI': 0.031900536683440256}\n",
      "------------------------------------epoch: 300------------------------------------\n",
      "\t\t\t\tcost: 0.6520341102384797\n",
      "b_dict: {'intercept': 0.15567719833985094, 'log_PFOS': 0.24068774191165115, 'age': 0.27402078337315944, 'gender': 0.3519518065244424, 'BMI': 0.033419126194305586}\n",
      "------------------------------------epoch: 400------------------------------------\n",
      "\t\t\t\tcost: 0.6520077339632842\n",
      "b_dict: {'intercept': 0.1558683927056963, 'log_PFOS': 0.24110140878865227, 'age': 0.2742957730383899, 'gender': 0.3521656703074247, 'BMI': 0.03366003763102537}\n",
      "---------------------------------------\n",
      "Reached a cost of 0.6520048125167088 after 493 epochs.\n",
      "final values: \n",
      "intercept: 0.15589\n",
      "log_PFOS: 0.24115527\n",
      "age: 0.27433078\n",
      "gender: 0.352183\n",
      "BMI: 0.03369477\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 1000\n",
    "cost = 0\n",
    "b_vector = np.zeros(X_with_intercept.shape[1]) ## initilize intercept and coefficents as 0\n",
    "cols = [\"intercept\",\"log_PFOS\", \"age\", \"gender\", \"BMI\"]\n",
    "\n",
    "\n",
    "def compute_sigmoid(B: np.ndarray, X: np.ndarray):\n",
    "\n",
    "    # return 1 / (1 + np.exp(-np.dot(X, B))) ## ALT METHOD: np.dot(X, B) computes the dot product between each row of X and B\n",
    "    return 1/(  1 + np.exp( - X @ B )  ) ## X @ B computes the dot product between each row of X and B\n",
    "\n",
    "\n",
    "def compute_cost(B: np.ndarray, X: np.ndarray, Y: np.ndarray):\n",
    "    num_rows = Y.shape[0]\n",
    "    sigmoid = compute_sigmoid(B, X)\n",
    "\n",
    "    return np.sum((Y * np.log(sigmoid)) + (1 - Y) * np.log(sigmoid)) / -num_rows\n",
    "\n",
    "def partial_derivative(B: np.ndarray, X: np.ndarray, Y: np.ndarray):\n",
    "    num_rows = Y.shape[0]\n",
    "    sigmoid = compute_sigmoid(B, X)\n",
    "\n",
    "    # return (1 / num_rows) * np.dot(X.T, (sigmoid - Y)) ## ALT METHOD\n",
    "    \n",
    "    ## (sigmoid - Y)[:,np.newaxis] changes its shape from (n,) to (n, 1)\n",
    "    return np.sum( (sigmoid - Y)[:,np.newaxis] * X, axis = 0 ) / num_rows\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    cost_old = cost\n",
    "    cost = compute_cost(b_vector, X_with_intercept, Y)\n",
    "    \n",
    "    if abs(cost_old - cost) >= 0.00000001:\n",
    "        b_vector -= LEARNING_RATE * partial_derivative(b_vector, X_with_intercept, Y)\n",
    "\n",
    "        b_dict = {col : b for col, b in zip(cols, b_vector)}\n",
    "\n",
    "        if epoch % 100 == 0 or epoch+1 == 1000:\n",
    "            print(f\"------------------------------------epoch: {epoch}------------------------------------\")\n",
    "            print(f\"\\t\\t\\t\\tcost: {cost}\\nb_dict: {b_dict}\")\n",
    "\n",
    "    else: break\n",
    "\n",
    "print(f\"---------------------------------------\\nReached a cost of {cost_old} after {epoch+1} epochs.\")\n",
    "print(\"final values: \")\n",
    "for key, value in b_dict.items(): print(f\"{key}: {round(value,8)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ecca7",
   "metadata": {},
   "source": [
    "(c) \n",
    "> (10 points) Apply your own algorithm to the standardized data and\n",
    "provide the values of the learned θ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9d8d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final intercept and coefficents: \n",
      "intercept: 0.15589\n",
      "log_PFOS: 0.24115527\n",
      "age: 0.27433078\n",
      "gender: 0.352183\n",
      "BMI: 0.03369477\n",
      "\n",
      "predicted y values:\n",
      " [-0.63767229 -0.35567773  0.28934299 -0.71980356 -0.22064488  1.3509815\n",
      " -0.93049029 -0.18919607 -0.26945483  0.31040522  0.28683061 -0.02218363\n",
      "  0.98500272  0.72087063 -0.2193257  -0.85167696  0.99214269  1.02340581\n",
      "  1.05475267 -0.26340078  0.41899288  0.22532663  0.01917964  0.2273359\n",
      " -0.87784797  0.37561435  0.61639175 -0.42788196 -0.0963243  -0.83015353\n",
      " -0.67437647 -0.50943761 -0.20147519 -0.2433236   0.12916858  0.40945369\n",
      " -0.14579274  0.90563315  0.21992537  0.39186762 -0.51365809 -0.83244135\n",
      "  0.02118992 -0.12097536  0.74917466 -0.02371512  1.22136204  0.12076554\n",
      "  0.94376059  0.91590366 -0.09737454  0.45757712  0.79814112 -0.54523908\n",
      "  0.11662007  0.36769692 -0.78482186 -0.21368869 -0.16293921 -0.06461144\n",
      " -1.33207721  1.07279839  0.66069954  0.22933911  1.09985611  0.95321847\n",
      "  0.63039275  0.14782821 -0.47249653 -0.16040713  0.17312329 -0.3855809\n",
      "  0.28343548  0.31051152  0.1148984   0.03723883  0.40413805 -0.23198394\n",
      "  0.4597564  -0.02521867  0.02514981  0.02285987  1.18952294  0.55849617\n",
      " -0.01045613 -0.07553636 -0.38839229  0.09100668  1.07800249  0.392383\n",
      "  0.7607487  -0.54622476 -0.45953832  0.33188588  0.34581291  0.83431568\n",
      " -0.34620721  0.14762807 -0.25290501 -0.01189399  0.3216334  -0.73251539\n",
      "  0.40629532  1.12335866  0.56532418  0.03669928  0.10386718  0.20303057\n",
      "  0.65737803 -0.45779377 -0.1143753   0.12276092  0.60681876  0.61788661\n",
      "  0.71353775 -0.01260523  0.06720701  0.54236867  0.45672539 -0.597136\n",
      "  0.12999838  0.18934142 -0.25473941  0.5419027  -0.7927713  -0.2343529\n",
      " -0.03344533  0.28057058  0.02355419  0.23482981  0.1149396  -1.10635988\n",
      "  0.08167491  0.18351284  0.96388594  0.44627531  0.59799111  0.39783854\n",
      "  0.73165551  0.34830256 -0.88102386  0.36842044  1.37219905  0.39932402\n",
      "  0.16438227  0.52601014  0.63291467  1.05115655 -0.15536693 -0.15545643\n",
      "  0.93441502  0.50801034 -0.04937338 -0.19255904 -0.50972658 -0.1795686\n",
      "  1.25311057  0.37346325  0.43156472 -0.09601647  0.15670061 -0.20263381\n",
      "  0.13326987  0.77751867  0.11490815 -0.95304043  0.35734564  0.66200015\n",
      "  0.78595262 -0.18374567  0.2604083   0.97713401  0.31027714  0.51513939\n",
      " -0.32770443 -0.29938882  0.28003478 -0.21432775  0.1899311  -0.39206057\n",
      "  0.03416342  0.04811718  1.13419689  0.41466336  0.9734647  -1.21429095\n",
      "  0.39868509 -0.34923937  0.41076909  0.48472853  0.08603452 -0.03692023\n",
      "  0.61244443  0.20194406 -0.29117951  0.05215665 -0.40290574 -0.28637157\n",
      "  0.3361431   0.66647711  0.80601999  0.15656382  0.26938012  0.19118954\n",
      "  0.11746336  0.12181432  0.26778846  0.92494461 -0.45694933 -0.15575985\n",
      " -0.76998197  0.11715904 -0.80019933 -0.80834121  0.62374795 -0.50611098\n",
      "  0.62706754  0.25895466  0.65602715 -0.4970739  -0.16692773  0.87920492\n",
      "  0.37237935  0.07267992  0.33858209 -0.80460997  0.03786836 -0.09614779\n",
      "  0.41131507 -0.57124516  0.11003069 -0.08885841  0.08951445 -0.26663455\n",
      "  0.61037621 -0.29723669 -0.71740647 -0.18197625 -0.52537412  0.02765553\n",
      "  0.21083791 -0.31137546  0.4556176   0.34176854  0.43314386  0.90498516\n",
      "  1.15651004  0.56517106  0.02917278 -0.64987896  1.27606081 -0.13339834\n",
      "  0.24356334  0.23786651 -0.56058523  0.97368573  0.36001667  0.57854353\n",
      "  0.71980084  1.29375504  0.45963104  0.27836935 -0.46132011  0.29560517\n",
      "  0.71352103  0.70532     0.55733247  0.15412637  0.09863887 -0.05645755\n",
      "  0.06066839 -0.55451918  0.15535081  0.6598713   0.51735073 -0.64470578\n",
      "  0.45769644 -0.18755719  0.18166911 -0.09074498  0.70039755 -0.33119294\n",
      " -0.58774845  0.50654877  0.88784392 -0.53473839  0.23897488  1.35860618\n",
      " -0.28862954  0.28726286 -0.63440104  0.51989926  0.00647326 -0.25270747\n",
      " -0.83430281  0.31190767  0.39748112  0.52917756  0.72997062  0.18708631]\n"
     ]
    }
   ],
   "source": [
    "## for each row (sample) in x, multiply the variables (x1-x4) by the weights in b\n",
    "## sum the resulting vector \n",
    "## this is the dot product \n",
    "\n",
    "print(\"final intercept and coefficents: \")\n",
    "for key, value in b_dict.items(): print(f\"{key}: {round(value,8)}\")\n",
    "\n",
    "y_predicted = np.dot( X_with_intercept, b_vector )\n",
    "# y_predicted = np.dot( X, b_vector[1: len(b_vector)] ) + b_vector[0] ## b_vector[0] is the intercept\n",
    "\n",
    "print(\"\\npredicted y values:\\n\", y_predicted )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e32b412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = np.where(y_predicted > 0, 1, 0) - Y\n",
    "len(diff[diff > 0]) / 300 ## 78 missclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061425f7",
   "metadata": {},
   "source": [
    "### Sklearn\n",
    "\n",
    "(d) \n",
    "> (10 points) Apply LogisticRegression in sklearn to the y and the stan-\n",
    "dardized x. What are the θ values you get from sklearn? Information\n",
    "about how to apply LogisticRegression in sklear can be found at\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.\n",
    "linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9485ded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficent: {'log_PFOS': 0.2415679442838867, 'age': 0.27431454819455997, 'gender': 0.3523996420149075, 'BMI': 0.03367537318410233}\n",
      "intercept: [0.1560249]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logreg = LogisticRegression(random_state=16,  max_iter=1000, penalty=None)\n",
    "\n",
    "# model = logreg.fit(X_df, Y_df)\n",
    "SklearnModel = logreg.fit(X, Y)\n",
    "\n",
    "coef_df = {col : coef for col, coef in zip(list(cols[1:len(cols)]), SklearnModel.coef_[0])}\n",
    "# print(coef_df)\n",
    "# print(X.columns)\n",
    "print(f\"coefficent: {coef_df}\\nintercept: {SklearnModel.intercept_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0e1bf",
   "metadata": {},
   "source": [
    "### statsmodel\n",
    "(e) \n",
    "> (10 points) Add constant to the standardized x using the function\n",
    "add constant. Instructions about how to use add constant can be\n",
    "found at:\n",
    "https://www.statsmodels.org/dev/generated/statsmodels.tools.tools.add_constant.html\n",
    "Apply Logit in statsmodels to the data with constant 1 added. What\n",
    "θ do you get? Instructions about how to use statsmodels to do logistic\n",
    "regression can be found at:\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.formula.api.logit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e0304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.658576\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>disease</td>     <th>  No. Observations:  </th>  <td>   300</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   295</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     4</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 07 Oct 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.04617</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:35:46</td>     <th>  Log-Likelihood:    </th> <td> -197.57</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -207.14</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.0007418</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.1559</td> <td>    0.120</td> <td>    1.303</td> <td> 0.193</td> <td>   -0.079</td> <td>    0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>       <td>    0.2412</td> <td>    0.123</td> <td>    1.963</td> <td> 0.050</td> <td>    0.000</td> <td>    0.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender</th>    <td>    0.2743</td> <td>    0.120</td> <td>    2.280</td> <td> 0.023</td> <td>    0.038</td> <td>    0.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BMI</th>       <td>    0.3522</td> <td>    0.123</td> <td>    2.868</td> <td> 0.004</td> <td>    0.112</td> <td>    0.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_PFOS</th>  <td>    0.0337</td> <td>    0.121</td> <td>    0.280</td> <td> 0.780</td> <td>   -0.203</td> <td>    0.270</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &     disease      & \\textbf{  No. Observations:  } &      300    \\\\\n",
       "\\textbf{Model:}           &      Logit       & \\textbf{  Df Residuals:      } &      295    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        4    \\\\\n",
       "\\textbf{Date:}            & Tue, 07 Oct 2025 & \\textbf{  Pseudo R-squ.:     } &  0.04617    \\\\\n",
       "\\textbf{Time:}            &     21:35:46     & \\textbf{  Log-Likelihood:    } &   -197.57   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -207.14   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } & 0.0007418   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &       0.1559  &        0.120     &     1.303  &         0.193        &       -0.079    &        0.390     \\\\\n",
       "\\textbf{age}       &       0.2412  &        0.123     &     1.963  &         0.050        &        0.000    &        0.482     \\\\\n",
       "\\textbf{gender}    &       0.2743  &        0.120     &     2.280  &         0.023        &        0.038    &        0.510     \\\\\n",
       "\\textbf{BMI}       &       0.3522  &        0.123     &     2.868  &         0.004        &        0.112    &        0.593     \\\\\n",
       "\\textbf{log\\_PFOS} &       0.0337  &        0.121     &     0.280  &         0.780        &       -0.203    &        0.270     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                disease   No. Observations:                  300\n",
       "Model:                          Logit   Df Residuals:                      295\n",
       "Method:                           MLE   Df Model:                            4\n",
       "Date:                Tue, 07 Oct 2025   Pseudo R-squ.:                 0.04617\n",
       "Time:                        21:35:46   Log-Likelihood:                -197.57\n",
       "converged:                       True   LL-Null:                       -207.14\n",
       "Covariance Type:            nonrobust   LLR p-value:                 0.0007418\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.1559      0.120      1.303      0.193      -0.079       0.390\n",
       "age            0.2412      0.123      1.963      0.050       0.000       0.482\n",
       "gender         0.2743      0.120      2.280      0.023       0.038       0.510\n",
       "BMI            0.3522      0.123      2.868      0.004       0.112       0.593\n",
       "log_PFOS       0.0337      0.121      0.280      0.780      -0.203       0.270\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = df.loc[:,[\"age\", \"gender\", \"BMI\", \"log_PFOS\"]]\n",
    "\n",
    "Y_df = df[\"disease\"]\n",
    "\n",
    "X_df_with_intercept = statsmodels.tools.tools.add_constant(X_df, prepend=True)\n",
    "model = api.logit(formula=\"disease ~ age + gender + BMI + log_PFOS\", data = pd.concat([X_df_with_intercept, Y_df], axis = 1))\n",
    "\n",
    "model.fit().summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d107ddd",
   "metadata": {},
   "source": [
    "> (f) (25 points) Compare θ from your own algorithm, θ from LogisticRegression in sklearn, and θ from statsmodel. Do you get very similar results? If not, what could you do to make the θ values similar?\n",
    "\n",
    "Below are the coefficents and intercepts for all of the models. \n",
    "\n",
    "**My final intercept and coefficents:**\n",
    "|Coefficent   | Value  |\n",
    "|-------------|--------|\n",
    "|intercept    |0.15589 |\n",
    "|log_PFOS     |0.241155|\n",
    "|age          |0.274331|\n",
    "|gender       |0.352183|\n",
    "|BMI          |0.033695|\n",
    "-----------------------------------------------------------------\n",
    "**Sklearn intercept and coefficents:**\n",
    "|Coefficent   | Value  |\n",
    "|-------------|--------|\n",
    "|intercept    |0.156025|\n",
    "|log_PFOS     |0.241568|\n",
    "|age          |0.274315|\n",
    "|gender       |0.3524  |\n",
    "|BMI          |0.033675|\n",
    "-----------------------------------------------------------------\n",
    "**StatsModel intercept and coefficents:**\n",
    "|Coefficent   | Value  |\n",
    "|-------------|--------|\n",
    "|Intercept    |0.155894|\n",
    "|age          |0.241166|\n",
    "|gender       |0.274338|\n",
    "|BMI          |0.352185|\n",
    "|log_PFOS     |0.033702|\n",
    "\n",
    "In order for sklearn's results to mirror statsmodel's and mine, I set `penalty=None` in sklearn's LogisticRegression model, this removed L1 Regularization. Afterwards,  the coefficents from sklearn and statsmodel were practically the same as my values which means my model performed just as well as the others. Looking at the tables above, it is evident that my coeffients were the same as theirs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129424e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My final intercept and coefficents:\n",
      "intercept: 0.15589\n",
      "log_PFOS: 0.241155\n",
      "age: 0.274331\n",
      "gender: 0.352183\n",
      "BMI: 0.033695\n",
      "-----------------------------------------------------------------\n",
      "Sklearn intercept and coefficents:\n",
      "intercept: 0.156025\n",
      "log_PFOS: 0.241568\n",
      "age: 0.274315\n",
      "gender: 0.3524\n",
      "BMI: 0.033675\n",
      "-----------------------------------------------------------------\n",
      "StatsModel intercept and coefficents:\n",
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.658576\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intercept    0.155894\n",
       "age          0.241166\n",
       "gender       0.274338\n",
       "BMI          0.352185\n",
       "log_PFOS     0.033702\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"My final intercept and coefficents:\")\n",
    "for key, value in b_dict.items(): print(f\"{key}: {round(value, 6)}\")\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"Sklearn intercept and coefficents:\")\n",
    "print(f\"intercept: {round(SklearnModel.intercept_[0], 6)}\")\n",
    "for key, value in coef_df.items(): print(f\"{key}: {round(value, 6)}\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"StatsModel intercept and coefficents:\\n\")\n",
    "model.fit().params\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
